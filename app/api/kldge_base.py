import requests
import json
import math
import os
import time
import subprocess
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

# Prometheus é…ç½®
PROMETHEUS_URL = os.getenv("PROMETHEUS_URL", "http://localhost:9090")

# ==========================================
# 1. å®šä¹‰æ•°æ®æ¨¡å‹
# ==========================================

class NodeOverview(BaseModel):
    physical_nodes: int | str
    virtual_nodes: int | str
    total_qps: float | str

class PerformanceMetrics(BaseModel):
    avg_response_time: float | str
    error_rate: float | str
    active_connections: int | str
    system_load: float | str

class NodeMetrics(BaseModel):
    instance: str
    weight: float
    target_weight: float
    connections: int | str
    qps: float | str
    response_time: float | str

class MetricsResponse(BaseModel):
    node_overview: NodeOverview
    performance_metrics: PerformanceMetrics
    realtime_monitoring: list[NodeMetrics]
    timestamp: str
    query_time_ms: int

# ==========================================
# 2. è¾…åŠ©å·¥å…·å‡½æ•°
# ==========================================

def query_prometheus(query):
    """æ‰§è¡Œ Prometheus æŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
    try:
        response = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={'query': query, 'time': time.time()},
            timeout=2
        )
        response.raise_for_status()
        data = response.json()
        if data['status'] == 'success':
            return data['data']['result']
    except Exception as e:
        print(f"--- [DEBUG] Prometheus æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e} ---", flush=True)
    return []

def format_value(value):
    """æ ¼å¼åŒ–æ•°å€¼ï¼Œä¿ç•™2ä½å°æ•°"""
    try:
        val = float(value)
        if math.isnan(val) or math.isinf(val):
            return 0
        if val.is_integer():
            return int(val)
        return round(val, 2)
    except (ValueError, TypeError):
        return 0

def validate_and_clean_data(data):
    """é€’å½’æ¸…ç†æ•°æ®ä¸­çš„ NaN å’Œ Infinity"""
    if isinstance(data, dict):
        return {k: validate_and_clean_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [validate_and_clean_data(i) for i in data]
    elif isinstance(data, float):
        if math.isnan(data) or math.isinf(data):
            return 0
        return data
    return data

# ==========================================
# 3. æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
# ==========================================

def get_docker_stats():
    """
    ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤ 'docker ps' è·å–å®¹å™¨åˆ—è¡¨
    è¿”å›: (pg_count, weaviate_count, first_pg_name)
    """
    try:
        cmd = ["docker", "ps", "--format", "{{.Image}}@{{.Names}}"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            return 0, 0, "Unknown-Node"

        output = result.stdout.strip()
        if not output:
            return 0, 0, "Unknown-Node"
            
        lines = output.split('\n')
        
        pg_count = 0
        weaviate_count = 0
        first_pg_name = "postgres-node-01" # é»˜è®¤å€¼

        for line in lines:
            line = line.strip()
            if not line or '@' not in line: 
                continue
                
            parts = line.split('@')
            image_name = parts[0].lower()
            container_name = parts[1]
            
            if 'postgres' in image_name:
                pg_count += 1
                if pg_count == 1:
                    first_pg_name = container_name
            elif 'weaviate' in image_name:
                weaviate_count += 1
            
        return pg_count, weaviate_count, first_pg_name

    except Exception as e:
        print(f"--- [DEBUG] Docker æ‰«æå‡ºé”™: {e} ---", flush=True)
        return 0, 0, "Unknown-Node"

def get_node_overview():
    """è·å–èŠ‚ç‚¹æ¦‚è§ˆ"""
    pg_count, weaviate_count, _ = get_docker_stats()
    
    total_qps = 0
    qps_data = query_prometheus('sum(rate(nginx_http_response_count_total[5m]))')
    if qps_data and len(qps_data) > 0:
        total_qps = format_value(qps_data[0]['value'][1])

    return {
        "physical_nodes": pg_count,
        "virtual_nodes": weaviate_count,
        "total_qps": total_qps
    }

def get_performance_metrics():
    """
    è·å–æ€§èƒ½æŒ‡æ ‡
    """
    metrics = {
        "avg_response_time": 0,
        "error_rate": 0,
        "active_connections": 0,
        "system_load": 0
    }
    
    # 1. å¹³å‡å“åº”æ—¶é—´
    resp_time = query_prometheus('sum(rate(nginx_http_upstream_time_seconds_sum{status="200"}[15m]))/sum(rate(nginx_http_upstream_time_seconds_count{status="200"}[15m]))')
    if resp_time: metrics["avg_response_time"] = format_value(resp_time[0]['value'][1])

    # 2. é”™è¯¯ç‡
    err_rate = query_prometheus('sum(rate(nginx_http_response_count_total{status=~"5.."}[15m])) / sum(rate(nginx_http_response_count_total[15m])) * 100')
    if err_rate: metrics["error_rate"] = format_value(err_rate[0]['value'][1])

    # 3. æ´»è·ƒè¿æ¥æ•° (ä½¿ç”¨æ—¥å¿—ä¼°ç®—å¹¶å‘æ•°)
    # ã€ç¬¬ä¸€é¡ºä½ã€‘ç›´æ¥æŸ¥è¯¢ Prometheus é‡Œçš„çœŸå®æŒ‡æ ‡ (ä¹Ÿå°±æ˜¯ä½ æˆªå›¾é‡ŒæŸ¥åˆ°çš„é‚£ä¸ª)
    real_active = query_prometheus('sum(nginx_connections_active)')
    
    if real_active:
        # å¦‚æœæŸ¥åˆ°äº†ï¼Œç›´æ¥ç”¨ï¼(æˆªå›¾é‡Œæ˜¯ 3ï¼Œè¿™é‡Œå°±ä¼šæ‹¿åˆ° 3)
        metrics["active_connections"] = int(float(real_active[0]['value'][1]))
    else:
        # ã€ç¬¬äºŒé¡ºä½ã€‘å¦‚æœæ²¡æŸ¥åˆ°ï¼Œå†è¿›è¡Œä¼°ç®— (Little's Law ä¿åº•)
        concurrency_data = query_prometheus('sum(rate(nginx_http_request_duration_seconds_sum[15m]))')
        calculated_conns = 0.0
        if concurrency_data:
            calculated_conns = float(concurrency_data[0]['value'][1])
        
        # ç®€å•çš„ä¿åº•é€»è¾‘
        if calculated_conns >= 1:
            metrics["active_connections"] = int(calculated_conns)
        elif calculated_conns > 0.001:
            metrics["active_connections"] = 1
        else:
            # å†çœ‹çœ‹æœ‰æ²¡æœ‰ QPSï¼Œæœ‰QPSè‡³å°‘ç»™ä¸ª1
            qps_check = query_prometheus('sum(rate(nginx_http_response_count_total[15m]))')
            if qps_check and float(qps_check[0]['value'][1]) > 0:
                 metrics["active_connections"] = 1
            else:
                 metrics["active_connections"] = 0
    
    # 4. ç³»ç»Ÿè´Ÿè½½ (CPUä½¿ç”¨ç‡)
    cpu_usage = query_prometheus('100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[15m])) * 100)')
    if cpu_usage:
        metrics["system_load"] = format_value(cpu_usage[0]['value'][1])
    else:
        load_avg = query_prometheus('avg(node_load1)')
        if load_avg: metrics["system_load"] = format_value(load_avg[0]['value'][1])

    return metrics

def assemble_realtime_metrics(node_overview, performance_metrics):
    """
    ã€æ ¸å¿ƒä¿®å¤ã€‘
    ä¸å†ç‹¬ç«‹æŸ¥è¯¢ï¼Œè€Œæ˜¯ç›´æ¥ã€å¤ç”¨ã€‘ä¸Šé¢çš„æ€»æ•°æ®ã€‚
    è¿™æ ·ä¿è¯ï¼šTop(1) == Bottom(1)ï¼Œç»ä¸å‡ºç°ä¸Šé¢æ˜¯1ä¸‹é¢æ˜¯0çš„æƒ…å†µã€‚
    """
    # å†æ¬¡è·å– Docker åå­—ï¼Œè®©åˆ—è¡¨æ˜¾ç¤ºçœŸå®çš„å®¹å™¨å (å¦‚ docker_fengfz-db-1)
    pg_count, _, first_pg_name = get_docker_stats()
    
    instance_name = first_pg_name if pg_count > 0 else "No-Active-Node"
    
    # æ„é€ å•ä¸ªèŠ‚ç‚¹ï¼Œç›´æ¥å¼•ç”¨ performance_metrics çš„å€¼
    single_node = {
        "instance": instance_name,
        "weight": 100, 
        "target_weight": 100,
        # ğŸ‘‡ğŸ‘‡ğŸ‘‡ å…³é”®ç‚¹ï¼šç›´æ¥èµ‹å€¼ ğŸ‘‡ğŸ‘‡ğŸ‘‡
        "connections": performance_metrics.get("active_connections", 0),
        "qps": node_overview.get("total_qps", 0),
        "response_time": performance_metrics.get("avg_response_time", 0)
    }
    
    return [single_node]

# ==========================================
# 4. API è·¯ç”±å…¥å£
# ==========================================

@router.get("/kldgebase/metrics", response_model=MetricsResponse)
def get_metrics():
    """è·å–æ‰€æœ‰æŒ‡æ ‡æ•°æ®"""
    try:
        start_time = time.time()
        
        # 1. å…ˆè®¡ç®—æ€»æŒ‡æ ‡
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_overview = executor.submit(get_node_overview)
            future_performance = executor.submit(get_performance_metrics)
            
            node_overview = future_overview.result()
            performance_metrics = future_performance.result()

        # 2. ã€å…³é”®ã€‘ç”¨æ€»æŒ‡æ ‡ç»„è£…åˆ—è¡¨ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        realtime_monitoring = assemble_realtime_metrics(node_overview, performance_metrics)

        query_time_ms = int((time.time() - start_time) * 1000)
        timestamp = datetime.now(timezone.utc).isoformat(timespec='milliseconds').replace('+00:00', 'Z')

        response_data = {
            "node_overview": node_overview,
            "performance_metrics": performance_metrics,
            "realtime_monitoring": realtime_monitoring,
            "timestamp": timestamp,
            "query_time_ms": query_time_ms
        }

        return validate_and_clean_data(response_data)

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))